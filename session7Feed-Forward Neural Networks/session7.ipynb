{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#retrieved from https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
        "#Jason Brownlee\n",
        "\n",
        "# Perceptron Algorithm on the Sonar Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "OF4FePm8Su3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02413f40-aa87-4509-8bf0-70b7709e96a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCgS3uClSW4F",
        "outputId": "2ebdc441-fc89-49d7-aae6-aef92d4feec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [98.34958739684922, 98.94973743435858, 98.49962490622656]\n",
            "Mean Accuracy: 98.600%\n"
          ]
        }
      ],
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        "\n",
        "# Make a prediction with weights\n",
        "def predict(row, weights):\n",
        "\tactivation = weights[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tactivation += weights[i + 1] * row[i]\n",
        "\treturn 1.0 if activation >= 0.0 else 0.0\n",
        "\n",
        "# Estimate Perceptron weights using stochastic gradient descent\n",
        "def train_weights(train, l_rate, n_epoch):\n",
        "\tweights = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tprediction = predict(row, weights)\n",
        "\t\t\terror = row[-1] - prediction\n",
        "\t\t\tweights[0] = weights[0] + l_rate * error\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "\treturn weights\n",
        "\n",
        "# Perceptron Algorithm With Stochastic Gradient Descent\n",
        "def perceptron(train, test, l_rate, n_epoch):\n",
        "\tpredictions = list()\n",
        "\tweights = train_weights(train, l_rate, n_epoch)\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(row, weights)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)\n",
        "\n",
        "# Test the Perceptron algorithm on the sonar dataset\n",
        "seed(1)\n",
        "np.random.seed(1)\n",
        "# load and prepare data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "mask_train = np.isin(y_train, [0, 1])\n",
        "mask_test = np.isin(y_test, [0, 1])\n",
        "X = np.concatenate([x_train[mask_train], x_test[mask_test]], axis=0)\n",
        "y = np.concatenate([y_train[mask_train], y_test[mask_test]], axis=0)\n",
        "n_per_class = 2000\n",
        "idx0 = np.where(y == 0)[0][:n_per_class]\n",
        "idx1 = np.where(y == 1)[0][:n_per_class]\n",
        "idx = np.concatenate([idx0, idx1])\n",
        "np.random.shuffle(idx)\n",
        "X = X[idx]\n",
        "y = y[idx]\n",
        "X = X.reshape((X.shape[0], -1)).astype('float32') / 255.0\n",
        "dataset = [list(X[i]) + [float(y[i])] for i in range(len(y))]\n",
        "# evaluate algorithm\n",
        "n_folds = 3\n",
        "l_rate = 0.01\n",
        "n_epoch = 50\n",
        "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
      ]
    }
  ]
}